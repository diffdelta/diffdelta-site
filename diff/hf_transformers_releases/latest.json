{
  "schema_version": "1.2.0",
  "generated_at": "2026-02-09T14:22:49Z",
  "source_id": "hf_transformers_releases",
  "hash": "sha256:e0f849e58097f1a07b1782aa06a1a5ef0e4821b0381cc3390dbf8ad4a75fb6d3",
  "prev_hash": "sha256:0000000000000000000000000000000000000000000000000000000000000000",
  "cursor": "sha256:6db581e4806e483d2ba537e464c41191c26168ea144303a9831a20a5cc9291de",
  "prev_cursor": "sha256:0000000000000000000000000000000000000000000000000000000000000000",
  "changed": true,
  "ttl_sec": 3600,
  "sources_included": [
    "hf_transformers_releases"
  ],
  "batch_narrative": "hf_transformers_releases: 20 releases (16 stable, 4 pre-release, 3 security patches).",
  "buckets": {
    "new": [
      {
        "source": "hf_transformers_releases",
        "id": "277347877",
        "url": "https://github.com/huggingface/transformers/releases/tag/v4.57.6",
        "published_at": "2026-01-16T10:40:02Z",
        "updated_at": "2026-01-16T10:40:02Z",
        "headline": "Patch release v4.57.6",
        "content": {
          "excerpt_text": "## What's Changed Another fix for qwen vl models that prevented correctly loading the associated model type - this works together with https://github.com/huggingface/transformers/pull/41808 of the previous patch release. * Fixed incorrect model_type for qwen2vl and qwen2.5vl when config is saved and loaded again by @i3hz in https://github.com/huggingface/transformers/pull/41758 **Full Changelog**: https://github.com/huggingface/transformers/compare/v4.57.5...v4.57.6",
          "lang": "en"
        },
        "provenance": {
          "fetched_at": "2026-02-09T14:22:49Z",
          "evidence_urls": [
            "https://github.com/huggingface/transformers/releases/tag/v4.57.6"
          ],
          "content_hash": "sha256:c43fe7a0fbefc94fe2f9414c990b7512430be34e0a6ea7bc203c23d1c41217fb"
        },
        "signals": {
          "release": {
            "version": "v4.57.6",
            "prerelease": false
          },
          "suggested_action": "REVIEW_CHANGELOG"
        }
      },
      {
        "source": "hf_transformers_releases",
        "id": "276357231",
        "url": "https://github.com/huggingface/transformers/releases/tag/v4.57.5",
        "published_at": "2026-01-13T13:29:13Z",
        "updated_at": "2026-01-13T13:29:13Z",
        "headline": "Patch release v4.57.5",
        "content": {
          "excerpt_text": "## What's Changed Should not have said last patch :wink: These should be the last remaining fixes that got lost in between patches and the transition to v5. * QwenVL: add skipped keys in setattr as well by @zucchini-nlp in https://github.com/huggingface/transformers/pull/41808 * Fix lr_scheduler_parsing by @SunMarc in https://github.com/huggingface/transformers/pull/41322 **Full Changelog**: https://github.com/huggingface/transformers/compare/v4.57.4...v4.57.5",
          "lang": "en"
        },
        "provenance": {
          "fetched_at": "2026-02-09T14:22:49Z",
          "evidence_urls": [
            "https://github.com/huggingface/transformers/releases/tag/v4.57.5"
          ],
          "content_hash": "sha256:fe1f1cdc3254074e3d13c6adef32689e2eceed17d489ddc56ca74359de89725d"
        },
        "signals": {
          "release": {
            "version": "v4.57.5",
            "prerelease": false
          },
          "suggested_action": "REVIEW_CHANGELOG"
        }
      },
      {
        "source": "hf_transformers_releases",
        "id": "276312241",
        "url": "https://github.com/huggingface/transformers/releases/tag/v4.57.4",
        "published_at": "2026-01-13T11:07:40Z",
        "updated_at": "2026-01-13T11:07:40Z",
        "headline": "Patch release v4.57.4",
        "content": {
          "excerpt_text": "## What's Changed Last patch release for v4: We have a few small fixes for remote generation methods (e.g. group beam search), vLLM, and an offline tokenizer fix (if it's already been cached). * Grouped beam search from config params by @zucchini-nlp in https://github.com/huggingface/transformers/pull/42472 * Handle decorator with optional arguments better @hmellor in https://github.com/huggingface/transformers/pull/42512 * fix: make mistral base check conditional to fix offline loading by...",
          "lang": "en"
        },
        "provenance": {
          "fetched_at": "2026-02-09T14:22:49Z",
          "evidence_urls": [
            "https://github.com/huggingface/transformers/releases/tag/v4.57.4"
          ],
          "content_hash": "sha256:e78c7150fda9d01a38e87651096c9489edd3287367202ca23fd917ba82b37a80"
        },
        "signals": {
          "release": {
            "version": "v4.57.4",
            "prerelease": false
          },
          "suggested_action": "REVIEW_CHANGELOG"
        }
      },
      {
        "source": "hf_transformers_releases",
        "id": "265176796",
        "url": "https://github.com/huggingface/transformers/releases/tag/v4.57.3",
        "published_at": "2025-11-25T15:51:36Z",
        "updated_at": "2025-11-25T15:51:36Z",
        "headline": "Patch release v4.57.3",
        "content": {
          "excerpt_text": "There was a hidden bug when loading models with `local_files_only=True` and a typo related to the recent patch. The main fix is: https://github.com/huggingface/transformers/commit/b6055550a15a8fab367cf983b743ff68cc58d81a. We are really sorry that this slipped through, our CIs just did not catch it. As it affects a lot of users we are gonna yank the previous release",
          "lang": "en"
        },
        "provenance": {
          "fetched_at": "2026-02-09T14:22:49Z",
          "evidence_urls": [
            "https://github.com/huggingface/transformers/releases/tag/v4.57.3"
          ],
          "content_hash": "sha256:647be6916439713be9235216bd7077ba423350440bde5cd8bc5d7a6b280962eb"
        },
        "signals": {
          "release": {
            "version": "v4.57.3",
            "prerelease": false
          },
          "suggested_action": "REVIEW_CHANGELOG"
        }
      },
      {
        "source": "hf_transformers_releases",
        "id": "264765393",
        "url": "https://github.com/huggingface/transformers/releases/tag/v4.57.2",
        "published_at": "2025-11-24T17:54:34Z",
        "updated_at": "2025-11-24T17:55:30Z",
        "headline": "Patch Release v4.57.2",
        "content": {
          "excerpt_text": "This patch most notably fixes an issue on some Mistral tokenizers. It contains the following commits: - Add AutoTokenizer mapping for mistral3 and ministral (#42198) - Auto convert tekken.json (#42299) - fix tekken pattern matching (#42363) - Check model inputs - hidden states (#40994) - Remove invalid `@staticmethod` from module-level get_device_and_memory_breakdown (#41747)",
          "lang": "en"
        },
        "provenance": {
          "fetched_at": "2026-02-09T14:22:49Z",
          "evidence_urls": [
            "https://github.com/huggingface/transformers/releases/tag/v4.57.2"
          ],
          "content_hash": "sha256:0af279bb26a150882fdde590f9b5e6dfe1e8f04b1ad4c437aff3830bfa1d4ea7"
        },
        "signals": {
          "release": {
            "version": "v4.57.2",
            "prerelease": false
          },
          "suggested_action": "REVIEW_CHANGELOG"
        }
      },
      {
        "source": "hf_transformers_releases",
        "id": "254417148",
        "url": "https://github.com/huggingface/transformers/releases/tag/v4.57.1",
        "published_at": "2025-10-14T15:39:34Z",
        "updated_at": "2025-10-14T15:39:34Z",
        "headline": "Patch release v4.57.1",
        "content": {
          "excerpt_text": "This patch most notably fixes an issue with an optional dependency (`optax`), which resulted in parsing errors with `poetry`. It contains the following fixes: - [fix optax dep issue](https://github.com/huggingface/transformers/commit/0645c9ec3188e000aecf5060e2cdabcc156bb794) - [remove offload_state_dict from kwargs](https://github.com/huggingface/transformers/commit/a92b1e8a45e1863b95c5e2caa12f5597aee80279) - Fix bnb fsdp loading for pre-quantized checkpoint (#41415) - Fix tests fsdp...",
          "lang": "en"
        },
        "provenance": {
          "fetched_at": "2026-02-09T14:22:49Z",
          "evidence_urls": [
            "https://github.com/huggingface/transformers/releases/tag/v4.57.1"
          ],
          "content_hash": "sha256:b8762adcc03effcb56183d85aaa9b65fbc6578b180dd01bf15bba5654da21424"
        },
        "signals": {
          "release": {
            "version": "v4.57.1",
            "prerelease": false
          },
          "suggested_action": "REVIEW_CHANGELOG"
        }
      },
      {
        "source": "hf_transformers_releases",
        "id": "247897629",
        "url": "https://github.com/huggingface/transformers/releases/tag/v4.56.2",
        "published_at": "2025-09-17T09:13:17Z",
        "updated_at": "2025-09-17T09:13:17Z",
        "headline": "Patch release v4.56.2",
        "content": {
          "excerpt_text": "- Processor load with multi-processing (#40786) - [Jetmoe] Fix RoPE (#40819) - Fix getter regression (#40824) - Fix config dtype parsing for Emu3 edge case (#40766)",
          "lang": "en"
        },
        "provenance": {
          "fetched_at": "2026-02-09T14:22:49Z",
          "evidence_urls": [
            "https://github.com/huggingface/transformers/releases/tag/v4.56.2"
          ],
          "content_hash": "sha256:f560e7241af9eb28477bf100daa2c438e3f6308be2df57da3f4a13fc6e2c8125"
        },
        "signals": {
          "release": {
            "version": "v4.56.2",
            "prerelease": false
          },
          "suggested_action": "REVIEW_CHANGELOG"
        }
      },
      {
        "source": "hf_transformers_releases",
        "id": "246822676",
        "url": "https://github.com/huggingface/transformers/releases/tag/v4.56.1-Vault-Gemma-preview",
        "published_at": "2025-09-12T15:43:49Z",
        "updated_at": "2025-09-12T15:43:49Z",
        "headline": "Vault-Gemma (based on v4.56.1)",
        "content": {
          "excerpt_text": "A new model is added to transformers: Vault-Gemma It is added on top of the v4.56.1 release, and can be installed from the following tag: `v4.56.1-Vault-Gemma-preview`. In order to install this version, please install with the following command: ``` pip install git+https://github.com/huggingface/transformers@v4.56.1-Vault-Gemma-preview ``` If fixes are needed, they will be applied to this release; this installation may therefore be considered as stable and improving. As the tag implies, this...",
          "lang": "en"
        },
        "provenance": {
          "fetched_at": "2026-02-09T14:22:49Z",
          "evidence_urls": [
            "https://github.com/huggingface/transformers/releases/tag/v4.56.1-Vault-Gemma-preview"
          ],
          "content_hash": "sha256:e285878a673475a78f7dc78ead2d3fe59a0db3a2d6c19bb5f8e3f983c33e39fb"
        },
        "signals": {
          "release": {
            "version": "v4.56.1-Vault-Gemma-preview",
            "prerelease": false
          },
          "suggested_action": "REVIEW_CHANGELOG"
        }
      },
      {
        "source": "hf_transformers_releases",
        "id": "244823353",
        "url": "https://github.com/huggingface/transformers/releases/tag/v4.56.0-Embedding-Gemma-preview",
        "published_at": "2025-09-04T15:53:11Z",
        "updated_at": "2025-09-05T15:50:54Z",
        "headline": "Embedding Gemma (based on v4.56.0)",
        "content": {
          "excerpt_text": "A new model is added to transformers: Embedding Gemma It is added on top of the v4.56.0 release, and can be installed from the following tag: `v4.56.0-Embedding-Gemma-preview`. In order to install this version, please install with the following command: ``` pip install git+https://github.com/huggingface/transformers@v4.56.0-Embedding-Gemma-preview ``` If fixes are needed, they will be applied to this release; this installation may therefore be considered as stable and improving. As the tag...",
          "lang": "en"
        },
        "provenance": {
          "fetched_at": "2026-02-09T14:22:49Z",
          "evidence_urls": [
            "https://github.com/huggingface/transformers/releases/tag/v4.56.0-Embedding-Gemma-preview"
          ],
          "content_hash": "sha256:5c65e236352ed1f83cab2d81d5caa2c308b3e8bb6e264286a74d900c1a3d8c87"
        },
        "signals": {
          "release": {
            "version": "v4.56.0-Embedding-Gemma-preview",
            "prerelease": false
          },
          "suggested_action": "REVIEW_CHANGELOG"
        }
      },
      {
        "source": "hf_transformers_releases",
        "id": "244911108",
        "url": "https://github.com/huggingface/transformers/releases/tag/v4.56.1",
        "published_at": "2025-09-04T20:47:27Z",
        "updated_at": "2025-09-04T20:49:05Z",
        "headline": "Patch release v4.56.1",
        "content": {
          "excerpt_text": "# Patch release v4.56.1 This patch most notably fixes an issue with the new `dtype` argument (replacing `torch_dtype`) in pipelines! ## Bug Fixes & Improvements - Fix broken Llama4 accuracy in MoE part (#40609) - fix pipeline dtype (#40638) - Fix self.dropout_p is not defined for SamAttention/Sam2Attention (#40667) - Fix backward compatibility with accelerate in Trainer (#40668) - fix broken offline mode when loading tokenizer from hub (#40669) - [Glm4.5V] fix vLLM support (#40696)",
          "lang": "en"
        },
        "provenance": {
          "fetched_at": "2026-02-09T14:22:49Z",
          "evidence_urls": [
            "https://github.com/huggingface/transformers/releases/tag/v4.56.1"
          ],
          "content_hash": "sha256:95cb3e168d8bc52b5bd186bf4e2c5f428312ea3b3084d6fba3ed13297ac65f4f"
        },
        "signals": {
          "release": {
            "version": "v4.56.1",
            "prerelease": false
          },
          "suggested_action": "REVIEW_CHANGELOG"
        }
      },
      {
        "source": "hf_transformers_releases",
        "id": "241875288",
        "url": "https://github.com/huggingface/transformers/releases/tag/v4.55.4",
        "published_at": "2025-08-22T15:18:46Z",
        "updated_at": "2025-08-25T11:13:37Z",
        "headline": "Patch v4.55.4",
        "content": {
          "excerpt_text": "# Patch v4.55.4 There was a mick mack on our side when cherry-picking the commit #40197 which led to a wrong commit in the patch! Sorry everyone \ud83d\ude2d This patch is just the official fix for #40197!",
          "lang": "en"
        },
        "provenance": {
          "fetched_at": "2026-02-09T14:22:49Z",
          "evidence_urls": [
            "https://github.com/huggingface/transformers/releases/tag/v4.55.4"
          ],
          "content_hash": "sha256:77e69cb0aff8af61b44db333ea919c62eb25d97b9aee2f0e8d2fa92d8f713ee8"
        },
        "signals": {
          "release": {
            "version": "v4.55.4",
            "prerelease": false
          },
          "suggested_action": "REVIEW_CHANGELOG"
        }
      },
      {
        "source": "hf_transformers_releases",
        "id": "241447746",
        "url": "https://github.com/huggingface/transformers/releases/tag/v4.55.3",
        "published_at": "2025-08-21T09:45:11Z",
        "updated_at": "2025-08-22T09:25:54Z",
        "headline": "Patch release v4.55.3",
        "content": {
          "excerpt_text": "# Patch release 4.55.3 Focused on stabilizing FlashAttention-2 on Ascend NPU, improving FSDP behavior for generic-task models, fixing MXFP4 integration for GPT-OSS ## Bug Fixes & Improvements - FlashAttention-2 / Ascend NPU \u2013 Fix \u201cunavailable\u201d runtime error (#40151) by @FightingZhen - FlashAttention kwargs \u2013 Revert FA kwargs preparation to resolve regression (#40161) by @Cyrilvallez - FSDP (generic-task models) \u2013 Fix sharding/runtime issues (#40191) by @Cyrilvallez - GPT-OSS /...",
          "lang": "en"
        },
        "provenance": {
          "fetched_at": "2026-02-09T14:22:49Z",
          "evidence_urls": [
            "https://github.com/huggingface/transformers/releases/tag/v4.55.3"
          ],
          "content_hash": "sha256:12bd837460a5a81ed5a131dd2def1fc55711051379d9f2a4f1e883146d4993cb"
        },
        "signals": {
          "release": {
            "version": "v4.55.3",
            "prerelease": false
          },
          "suggested_action": "REVIEW_CHANGELOG"
        }
      }
    ],
    "updated": [],
    "removed": [],
    "flagged": [
      {
        "source": "hf_transformers_releases",
        "id": "283382577",
        "url": "https://github.com/huggingface/transformers/releases/tag/v5.1.0",
        "published_at": "2026-02-05T15:44:54Z",
        "updated_at": "2026-02-05T15:44:54Z",
        "headline": "v5.1.0: EXAONE-MoE, PP-DocLayoutV3, Youtu-LLM, GLM-OCR",
        "content": {
          "excerpt_text": "## New Model additions ### EXAONE-MoE K-EXAONE is a large-scale multilingual language model developed by LG AI Research. Built using a Mixture-of-Experts architecture, K-EXAONE features 236 billion total parameters, with 23 billion active during inference. Performance evaluations across various benchmarks demonstrate that K-EXAONE excels in reasoning, agentic capabilities, general knowledge, multilingual understanding, and long-context processing. * Add EXAONE-MoE implementations (#43080) by...",
          "lang": "en"
        },
        "provenance": {
          "fetched_at": "2026-02-09T14:22:49Z",
          "evidence_urls": [
            "https://github.com/huggingface/transformers/releases/tag/v5.1.0"
          ],
          "content_hash": "sha256:5fdfc910bed0e5fa029720286d8620957d6c6c056e606832f46bb1fa72b7177a"
        },
        "signals": {
          "release": {
            "version": "v5.1.0",
            "prerelease": false
          },
          "deprecation": {
            "type": "breaking_change",
            "affects": [
              "SDPA",
              "num_frames",
              "ipex",
              "torchao.autoquant",
              "FbgemmFp8LinearTest"
            ],
            "confidence": "high",
            "source": "keyword_scan"
          },
          "suggested_action": "VERSION_PIN"
        }
      },
      {
        "source": "hf_transformers_releases",
        "id": "279872120",
        "url": "https://github.com/huggingface/transformers/releases/tag/v5.0.0",
        "published_at": "2026-01-26T10:17:10Z",
        "updated_at": "2026-01-26T10:42:16Z",
        "headline": "Transformers v5",
        "content": {
          "excerpt_text": "## Transformers v5 release notes - Highlights - Significant API changes: dynamic weight loading, tokenization - Backwards Incompatible Changes - Bugfixes and improvements We have a migration guide that will be continuously updated available on the `main` branch, please check it out in case you're facing issues: [migration guide](https://github.com/huggingface/transformers/blob/main/MIGRATION_GUIDE_V5.md). ## Highlights We are excited to announce the initial release of Transformers v5. This...",
          "lang": "en"
        },
        "provenance": {
          "fetched_at": "2026-02-09T14:22:49Z",
          "evidence_urls": [
            "https://github.com/huggingface/transformers/releases/tag/v5.0.0"
          ],
          "content_hash": "sha256:aa7648ba3826753a6b22596e61a89022dec856cfb5488ea1f01fb4a810080ca4"
        },
        "signals": {
          "release": {
            "version": "v5.0.0",
            "prerelease": false,
            "security_patch": true
          },
          "deprecation": {
            "type": "breaking_change",
            "affects": [
              "legacy",
              "altogether",
              "T5-specific",
              "tied",
              "them"
            ],
            "confidence": "high",
            "source": "keyword_scan"
          },
          "suggested_action": "PATCH_SOON"
        }
      },
      {
        "source": "hf_transformers_releases",
        "id": "276793530",
        "url": "https://github.com/huggingface/transformers/releases/tag/v5.0.0rc3",
        "published_at": "2026-01-26T10:02:55Z",
        "updated_at": "2026-01-26T10:02:55Z",
        "headline": "Release candidate v5.0.0rc3",
        "content": {
          "excerpt_text": "# Release candidate v5.0.0rc3 ## New models: * [GLM-4.7] GLM-Lite Supoort by @zRzRzRzRzRzRzR in https://github.com/huggingface/transformers/pull/43031 * [GLM-Image] AR Model Support for GLM-Image by @zRzRzRzRzRzRzR in https://github.com/huggingface/transformers/pull/43100 * Add LWDetr model by @sbucaille in https://github.com/huggingface/transformers/pull/40991 * Add LightOnOCR model implementation by @baptiste-aubertin in https://github.com/huggingface/transformers/pull/41621 ## What's...",
          "lang": "en"
        },
        "provenance": {
          "fetched_at": "2026-02-09T14:22:49Z",
          "evidence_urls": [
            "https://github.com/huggingface/transformers/releases/tag/v5.0.0rc3"
          ],
          "content_hash": "sha256:d7ee8aeb4d3b339af1aeee567ad31168a9236e3227b0a909ef26c20e5352fa00"
        },
        "signals": {
          "release": {
            "version": "v5.0.0rc3",
            "prerelease": true
          },
          "deprecation": {
            "type": "removal",
            "affects": [
              "stuff",
              "duplicate",
              "practices",
              "redundant",
              "classes"
            ],
            "confidence": "high",
            "source": "keyword_scan"
          },
          "suggested_action": "VERSION_PIN"
        }
      },
      {
        "source": "hf_transformers_releases",
        "id": "275106284",
        "url": "https://github.com/huggingface/transformers/releases/tag/v5.0.0rc2",
        "published_at": "2026-01-08T10:33:33Z",
        "updated_at": "2026-01-08T10:36:27Z",
        "headline": "Release candidate 5.0.0rc2",
        "content": {
          "excerpt_text": "## What's Changed This release candidate is focused on fixing `AutoTokenizer`, expanding the dynamic weight loading support, and improving performances with MoEs! ## MoEs and performances: * batched and grouped experts implementations by @IlyasMoutawwakil in https://github.com/huggingface/transformers/pull/42697 * Optimize MoEs for decoding using batched_mm by @IlyasMoutawwakil in https://github.com/huggingface/transformers/pull/43126 ## Tokenization: The main issue with the tokenization...",
          "lang": "en"
        },
        "provenance": {
          "fetched_at": "2026-02-09T14:22:49Z",
          "evidence_urls": [
            "https://github.com/huggingface/transformers/releases/tag/v5.0.0rc2"
          ],
          "content_hash": "sha256:86f80a3952c0c39502e1765cb63c193aa6ae257c0fe4e8200464674b462967a3"
        },
        "signals": {
          "release": {
            "version": "v5.0.0rc2",
            "prerelease": true
          },
          "deprecation": {
            "type": "breaking_change",
            "affects": [
              "tied"
            ],
            "confidence": "low",
            "source": "keyword_scan"
          },
          "suggested_action": "VERSION_PIN"
        }
      },
      {
        "source": "hf_transformers_releases",
        "id": "275100874",
        "url": "https://github.com/huggingface/transformers/releases/tag/v5.0.0rc1",
        "published_at": "2026-01-08T10:15:16Z",
        "updated_at": "2026-01-08T10:16:28Z",
        "headline": "Release candidate 5.0.0rc1",
        "content": {
          "excerpt_text": "## What's Changed This release candidate was focused mostly on `quantization` support with the new dynamic weight loader, and a few notable \ud83d\udea8 breaking changes\ud83d\udea8: 1. Default dtype for any model when using `from_pretrained` is now `auto`! * Default auto \ud83d\udea8 \ud83d\udea8 by @ArthurZucker in https://github.com/huggingface/transformers/pull/42805 2. Default shard size when saving a model is now 50GB: * \ud83d\udea8\ud83d\udea8 [saving] Default to 50GB shards, and remove non-safe serialization by @Cyrilvallez in...",
          "lang": "en"
        },
        "provenance": {
          "fetched_at": "2026-02-09T14:22:49Z",
          "evidence_urls": [
            "https://github.com/huggingface/transformers/releases/tag/v5.0.0rc1"
          ],
          "content_hash": "sha256:e55c5803c6a8ff7f98fee90ec09e9e55cf18d96ebf2e8db6133d771f1e36cca9"
        },
        "signals": {
          "release": {
            "version": "v5.0.0rc1",
            "prerelease": true
          },
          "deprecation": {
            "type": "breaking_change",
            "affects": [
              "non-safe"
            ],
            "confidence": "low",
            "source": "keyword_scan"
          },
          "suggested_action": "VERSION_PIN"
        }
      },
      {
        "source": "hf_transformers_releases",
        "id": "265932383",
        "url": "https://github.com/huggingface/transformers/releases/tag/v5.0.0rc0",
        "published_at": "2025-12-01T18:14:54Z",
        "updated_at": "2025-12-12T08:55:01Z",
        "headline": "Transformers v5.0.0rc0",
        "content": {
          "excerpt_text": "## Transformers v5 release notes - Highlights - Significant API changes: dynamic weight loading, tokenization - Backwards Incompatible Changes - Bugfixes and improvements ## Highlights We are excited to announce the initial release of Transformers v5. This is the first major release in five years, and the release is significant: 800 commits have been pushed to `main` since the latest minor release. This release removes a lot of long-due deprecations, introduces several refactors that...",
          "lang": "en"
        },
        "provenance": {
          "fetched_at": "2026-02-09T14:22:49Z",
          "evidence_urls": [
            "https://github.com/huggingface/transformers/releases/tag/v5.0.0rc0"
          ],
          "content_hash": "sha256:27ee2d3279236aae5146c231320fffcae7741ae583e165c6f96bf2e8c8769d7d"
        },
        "signals": {
          "release": {
            "version": "v5.0.0rc0",
            "prerelease": true,
            "security_patch": true
          },
          "deprecation": {
            "type": "breaking_change",
            "affects": [
              "legacy",
              "altogether",
              "T5-specific",
              "them",
              "_eventually_correct_t5_max_length"
            ],
            "confidence": "high",
            "source": "keyword_scan"
          },
          "suggested_action": "PATCH_SOON"
        }
      },
      {
        "source": "hf_transformers_releases",
        "id": "251936992",
        "url": "https://github.com/huggingface/transformers/releases/tag/v4.57.0",
        "published_at": "2025-10-03T17:04:51Z",
        "updated_at": "2025-10-03T17:04:51Z",
        "headline": "v4.57.0: Qwen3-Next, Vault Gemma, Qwen3 VL, LongCat Flash, Flex OLMO, LFM2 VL, BLT, Qwen3 OMNI MoE, Parakeet, EdgeTAM, OLMO3",
        "content": {
          "excerpt_text": "## New model additions ### Qwen3 Next The Qwen3-Next series represents the Qwen team's next-generation foundation models, optimized for extreme context length and large-scale parameter efficiency. The series introduces a suite of architectural innovations designed to maximize performance while minimizing computational cost: - **Hybrid Attention**: Replaces standard attention with the combination of **Gated DeltaNet** and **Gated Attention**, enabling efficient context modeling. -...",
          "lang": "en"
        },
        "provenance": {
          "fetched_at": "2026-02-09T14:22:49Z",
          "evidence_urls": [
            "https://github.com/huggingface/transformers/releases/tag/v4.57.0"
          ],
          "content_hash": "sha256:6a44bfdf7fed17bdd113caab89a591e1eb579975dd68cbbe9ba7d530993999b1"
        },
        "signals": {
          "release": {
            "version": "v4.57.0",
            "prerelease": false
          },
          "deprecation": {
            "type": "breaking_change",
            "affects": [
              "Group",
              "Constrained",
              "redundant",
              "unnecessary",
              "random"
            ],
            "confidence": "high",
            "source": "keyword_scan"
          },
          "suggested_action": "VERSION_PIN"
        }
      },
      {
        "source": "hf_transformers_releases",
        "id": "243094468",
        "url": "https://github.com/huggingface/transformers/releases/tag/v4.56.0",
        "published_at": "2025-08-29T18:24:00Z",
        "updated_at": "2025-08-29T18:24:00Z",
        "headline": "v4.56: Dino v3, X-Codec, Ovis 2, MetaCLIP 2, Florence 2, SAM 2, Kosmos 2.5, HunYuan, GLMV-4.5",
        "content": {
          "excerpt_text": "## New model additions ### Dino v3 DINOv3 is a family of versatile vision foundation models that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models. You can find all the original DINOv3 checkpoints under the...",
          "lang": "en"
        },
        "provenance": {
          "fetched_at": "2026-02-09T14:22:49Z",
          "evidence_urls": [
            "https://github.com/huggingface/transformers/releases/tag/v4.56.0"
          ],
          "content_hash": "sha256:e308b22c60c2070fa65628df47f527d4a2c6ddf255b12ac8e9f86e00f9e4a067"
        },
        "signals": {
          "release": {
            "version": "v4.56.0",
            "prerelease": false,
            "security_patch": true
          },
          "deprecation": {
            "type": "breaking_change",
            "affects": [
              "tensorflow",
              "decoding",
              "DoLa",
              "Contrastive",
              "triton_kernels"
            ],
            "confidence": "high",
            "source": "keyword_scan"
          },
          "suggested_action": "PATCH_SOON"
        }
      }
    ]
  },
  "counts": {
    "new": 12,
    "updated": 0,
    "removed": 0,
    "flagged": 8
  },
  "integrity_reset": true,
  "archive_url": "/archive/hf_transformers_releases/2026/02/09/20260209T142249Z_e0f849e58097f1a0.json"
}