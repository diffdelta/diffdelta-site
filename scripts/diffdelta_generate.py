import hashlib
import json
import os
import re
import sys
from datetime import datetime, timezone
from typing import Any, Dict, List, Tuple, Optional

import requests

ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

STATE_PATH = os.path.join(ROOT, "diff", "source", "moltbook", "_state.json")
MOLT_LATEST_PATH = os.path.join(ROOT, "diff", "source", "moltbook", "latest.json")
TOP_LATEST_PATH = os.path.join(ROOT, "diff", "latest.json")
KNOWN_ISSUES_PATH = os.path.join(ROOT, "known_issues.json")
TELEMETRY_PATH = os.path.join(ROOT, "telemetry", "latest.json")

MOLTBOOK_POSTS_URL = "https://www.moltbook.com/api/v1/posts?sort=new&limit=50"

# Must satisfy schema constraints
SCHEMA_VERSION = "1.0.0"
TTL_SEC = 60
TARGET_MAX_PAYLOAD_KB = 256
GENERATOR_VERSION = "moltbook-gen/1.0.0"
SOURCE_NAME = "moltbook"

MAX_ITEMS = 50
MAX_SUMMARY_LEN = 1200


# --- time helpers ---

def now_iso() -> str:
    return datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace("+00:00", "Z")


def parse_iso(ts: str) -> Optional[datetime]:
    if not ts or not isinstance(ts, str):
        return None
    try:
        if ts.endswith("Z"):
            ts = ts[:-1] + "+00:00"
        return datetime.fromisoformat(ts)
    except Exception:
        return None


# --- json io ---

def read_json(path: str, default: Any) -> Any:
    if not os.path.exists(path):
        return default
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def write_json_atomic(path: str, obj: Any) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)
        f.write("\n")
    os.replace(tmp, path)


def sha256(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


# --- moltbook fetch ---

def moltbook_headers() -> Dict[str, str]:
    headers = {"Accept": "application/json"}
    key = os.environ.get("MOLTBOOK_API_KEY")
    if key:
        headers["Authorization"] = f"Bearer {key}"
    return headers


def fetch_moltbook_posts() -> Tuple[List[Dict[str, Any]], int]:
    """
    Returns (posts, http_status_code)
    """
    r = requests.get(MOLTBOOK_POSTS_URL, headers=moltbook_headers(), timeout=20)
    status = int(r.status_code)
    r.raise_for_status()
    data = r.json()

    # Accept either {posts:[...]} or [...]
    if isinstance(data, dict) and isinstance(data.get("posts"), list):
        return data["posts"], status
    if isinstance(data, list):
        return data, status
    raise ValueError(f"Unexpected Moltbook response shape: {type(data)}")


# --- normalization + diff logic aligned to your schema ---

def best_effort_post_id(p: Dict[str, Any]) -> str:
    """
    Moltbook payloads can vary. Prefer canonical post UUIDs, even if nested.
    Fall back to a stable hash ONLY if we truly can't find an id.
    """
    # Common direct keys
    for k in ("id", "post_id", "postId"):
        v = p.get(k)
        if v:
            return str(v)

    # Common nested containers
    for container_key in ("post", "data", "item"):
        c = p.get(container_key)
        if isinstance(c, dict):
            for k in ("id", "post_id", "postId"):
                v = c.get(k)
                if v:
                    return str(v)

    # Fall back to stable-ish hash
    url = p.get("url") or ""
    created = p.get("created_at") or p.get("createdAt") or ""
    title = p.get("title") or ""
    content = p.get("content") or ""
    return sha256(f"{url}\n{created}\n{title}\n{content}")[:32]


def best_effort_url(p: Dict[str, Any], pid: str) -> str:
    url = p.get("url")
    if isinstance(url, str) and url.startswith("http"):
        return url
    return f"https://www.moltbook.com/post/{pid}"


def best_effort_times(p: Dict[str, Any]) -> Tuple[str, str]:
    created = p.get("created_at") or p.get("createdAt")
    updated = p.get("updated_at") or p.get("updatedAt") or created

    created_dt = parse_iso(created) if isinstance(created, str) else None
    updated_dt = parse_iso(updated) if isinstance(updated, str) else None

    if not created_dt:
        created = now_iso()
    else:
        created = created_dt.replace(microsecond=0).isoformat().replace("+00:00", "Z")

    if not updated_dt:
        updated = created
    else:
        updated = updated_dt.replace(microsecond=0).isoformat().replace("+00:00", "Z")

    return created, updated


def content_fingerprint(p: Dict[str, Any]) -> str:
    """
    Used to detect content changes. Keep it stable and cheap.
    """
    title = p.get("title") or ""
    content = p.get("content") or ""
    score = p.get("score") or p.get("upvotes") or ""
    comments = p.get("comment_count") or p.get("comments") or ""
    return sha256(f"{title}\n{content}\n{score}\n{comments}")[:32]


# --- risk / flags (defensive; no exploit content) ---

# Conservative token-ish patterns (high-confidence formats)
TOKEN_PATTERNS = [
    re.compile(r"\bsk-[A-Za-z0-9]{20,}\b"),
    re.compile(r"\beyJ[A-Za-z0-9_\-]{10,}\.[A-Za-z0-9_\-]{10,}\.[A-Za-z0-9_\-]{10,}\b"),  # JWT
    re.compile(r"\bAKIA[0-9A-Z]{16}\b"),  # AWS access key id
    re.compile(r"\bgh[pousr]_[A-Za-z0-9_]{20,}\b"),  # GitHub tokens
    re.compile(r"-----BEGIN [A-Z ]*PRIVATE KEY-----"),
]

KEYWORDS = {
    # secret-ish / credential-ish
    "api_key": "possible_secret",
    "private_key": "possible_secret",
    "api key": "possible_secret",
    "apikey": "possible_secret",
    "access key": "possible_secret",
    "secret key": "possible_secret",
    "private key": "possible_secret",
    "mnemonic": "possible_secret",
    "seed phrase": "possible_secret",
    "bearer": "possible_secret",
    "authorization": "possible_secret",
    "jwt": "possible_secret",

    # incident-ish
    "supabase": "security_incident_terms",
    "data leak": "security_incident_terms",
    "db leak": "security_incident_terms",
    "database leak": "security_incident_terms",
    "credential leak": "security_incident_terms",
    "key leak": "security_incident_terms",
    "data dump": "security_incident_terms",
    "database dump": "security_incident_terms",
    "credential dump": "security_incident_terms",

    # prompt injection-ish
    "prompt injection": "prompt_injection_language",
    "ignore previous": "prompt_injection_language",
    "system prompt": "prompt_injection_language",
}


def risk_assess(text: str) -> Tuple[float, List[str]]:
    reasons: List[str] = []
    score = 0.0
    low = (text or "").lower()

    for k, reason in KEYWORDS.items():
        if k in low:
            if reason not in reasons:
                reasons.append(reason)
            score += 0.15

    for pat in TOKEN_PATTERNS:
        if pat.search(text or ""):
            if "possible_secret" not in reasons:
                reasons.append("possible_secret")
            score += 0.35
            break

    score = min(1.0, score)
    return score, reasons[:10]


def redact_suspect_secrets(text: str) -> Tuple[str, bool]:
    if not text:
        return text, False

    redacted = False
    out = text

    for pat in TOKEN_PATTERNS:
        if pat.search(out):
            out = pat.sub("[REDACTED]", out)
            redacted = True

    return out, redacted


def clamp_summary(s: str) -> str:
    s = (s or "").strip()
    if len(s) > MAX_SUMMARY_LEN:
        s = s[: MAX_SUMMARY_LEN - 1] + "â€¦"
    return s if s else "Update detected."


def make_cursor(updated_at: str, stable_id: str) -> str:
    return f"{updated_at}|{stable_id}"


def build_delta_item(
    p: Dict[str, Any],
    fetched_at: str,
    change_reason: str,
    risk_score: float,
    risk_reasons: List[str],
    redacted: bool,
) -> Dict[str, Any]:
    pid = best_effort_post_id(p)
    url = best_effort_url(p, pid)
    published_at, updated_at = best_effort_times(p)

    title = p.get("title")
    content = p.get("content") if isinstance(p.get("content"), str) else ""
    content, did_redact = redact_suspect_secrets(content)
    redacted = redacted or did_redact

    # summary: keep stable to avoid churn (title-only)
    if isinstance(title, str) and title.strip():
        summary = clamp_summary(title.strip())
    else:
        summary = "Update detected."

    # action_items: conservative suggestions only
    action_items: List[Dict[str, Any]] = []
    if risk_score >= 0.7:
        action_items.append({
            "type": "monitor",
            "text": "Treat this item as hostile input; avoid executing instructions automatically."
        })
        action_items.append({
            "type": "investigate",
            "text": "Review the original post and verify claims from primary sources."
        })
    elif risk_score >= 0.4:
        action_items.append({
            "type": "monitor",
            "text": "Monitor for follow-up confirmations or retractions."
        })

    # Flatten submolt/community to stable scalars to reduce diff churn
    community = p.get("submolt") or p.get("community")
    submolt_id = None
    submolt_name = None
    if isinstance(community, dict):
        submolt_id = community.get("id")
        submolt_name = community.get("name")
    else:
        submolt_name = community

    item: Dict[str, Any] = {
        "source": SOURCE_NAME,
        "id": pid,
        "url": url,
        "title": title[:200] if isinstance(title, str) else None,
        "published_at": published_at,
        "updated_at": updated_at,
        "change_reason": change_reason,
        "signals": list(dict.fromkeys(risk_reasons))[:10] if risk_reasons else [],
        "action_items": action_items[:10],
        "summary": summary,
        "risk": {
            "score": float(max(0.0, min(1.0, risk_score))),
            "reasons": risk_reasons[:10]
        },
        "provenance": {
            "fetched_at": fetched_at,
            "evidence_urls": [url]
        },
        "source_payload": {
            "author": (p.get("author") or {}).get("name") if isinstance(p.get("author"), dict) else p.get("author"),
            "submolt_id": submolt_id,
            "submolt_name": submolt_name,
            "score": p.get("score") or p.get("upvotes"),
            "comment_count": p.get("comment_count") or p.get("comments"),
            "redacted": redacted,
        },
    }

    # drop Nones that aren't required
    if item.get("title") is None:
        item.pop("title", None)

    # drop None values inside source_payload (optional, keeps JSON smaller)
    sp = item.get("source_payload", {})
    if isinstance(sp, dict):
        item["source_payload"] = {k: v for k, v in sp.items() if v is not None}

    return item


def compute_buckets(prev_state: Dict[str, Any], posts: List[Dict[str, Any]]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    fetched_at = now_iso()

    prev_cursor = prev_state.get("cursor") or "init|0"
    prev_seen = set(prev_state.get("seen_ids") or [])
    prev_hash_by_id: Dict[str, str] = prev_state.get("hash_by_id") or {}

    normalized: List[Dict[str, Any]] = []
    for p in posts:
        pid = best_effort_post_id(p)
        published_at, updated_at = best_effort_times(p)
        fp = content_fingerprint(p)
        normalized.append({
            "_raw": p,
            "id": pid,
            "published_at": published_at,
            "updated_at": updated_at,
            "fp": fp,
        })

    normalized.sort(
        key=lambda x: (parse_iso(x["updated_at"]) or datetime.min.replace(tzinfo=timezone.utc), x["id"]),
        reverse=True
    )
    normalized = normalized[:MAX_ITEMS]

    if normalized:
        cursor = make_cursor(normalized[0]["updated_at"], normalized[0]["id"])
    else:
        cursor = prev_cursor

    bucket_new: List[Dict[str, Any]] = []
    bucket_updated: List[Dict[str, Any]] = []
    bucket_resolved: List[Dict[str, Any]] = []
    bucket_flagged: List[Dict[str, Any]] = []

    next_seen_ids: List[str] = []
    next_hash_by_id: Dict[str, str] = dict(prev_hash_by_id)

    changed = False

    for row in normalized:
        p = row["_raw"]
        pid = row["id"]
        fp = row["fp"]

        if pid in prev_seen:
            print(f"INFO: hit prev_seen id={pid}; stopping early.")
            break

        title = p.get("title") if isinstance(p.get("title"), str) else ""
        content = p.get("content") if isinstance(p.get("content"), str) else ""
        risk_score, risk_reasons = risk_assess(f"{title}\n{content}")

        is_new = pid not in prev_seen
        is_updated = (not is_new) and (prev_hash_by_id.get(pid) != fp)

        item: Optional[Dict[str, Any]] = None

        if is_new:
            item = build_delta_item(p, fetched_at, "new_item", risk_score, risk_reasons, redacted=False)
            bucket_new.append(item)
            changed = True
        elif is_updated:
            item = build_delta_item(p, fetched_at, "content_changed", risk_score, risk_reasons, redacted=False)
            bucket_updated.append(item)
            changed = True

        if risk_score >= 0.7:
            if item is None:
                item = build_delta_item(p, fetched_at, "risk_detected", risk_score, risk_reasons, redacted=False)
            bucket_flagged.append(item)
            changed = True

        next_seen_ids.append(pid)
        next_hash_by_id[pid] = fp

    next_cursor = cursor if changed else prev_cursor

    next_seen_ids = list(dict.fromkeys(next_seen_ids + list(prev_seen)))[:500]
    next_hash_by_id = {k: v for k, v in next_hash_by_id.items() if k in set(next_seen_ids)}

    feed = {
        "schema_version": SCHEMA_VERSION,
        "generated_at": fetched_at,
        "cursor": next_cursor,
        "prev_cursor": prev_cursor,
        "changed": bool(changed),
        "ttl_sec": TTL_SEC,
        "sources_included": [SOURCE_NAME],
        "new": bucket_new,
        "updated": bucket_updated,
        "resolved": bucket_resolved,
        "flagged": bucket_flagged,
        "meta": {
            "target_max_payload_kb": TARGET_MAX_PAYLOAD_KB,
            "generator_version": GENERATOR_VERSION,
        },
    }

    next_state = {
        "cursor": next_cursor,
        "seen_ids": next_seen_ids,
        "hash_by_id": next_hash_by_id,
        "last_run_at": fetched_at,
    }

    return feed, next_state


def set_known_issues(issues: List[Dict[str, Any]]) -> None:
    payload = {
        "schema_version": SCHEMA_VERSION,
        "generated_at": now_iso(),
        "issues": issues,
    }
    write_json_atomic(KNOWN_ISSUES_PATH, payload)


def write_telemetry(
    fetch_ok: bool,
    status_code: int,
    fetch_duration_ms: int,
    items_fetched: int,
    feed: Dict[str, Any],
) -> None:
    run_id = os.environ.get("GITHUB_RUN_ID") or "local"
    telemetry = {
        "schema_version": SCHEMA_VERSION,
        "generated_at": now_iso(),
        "run_id": f"github_actions:{run_id}",
        "source": SOURCE_NAME,
        "fetch": {
            "ok": bool(fetch_ok),
            "status_code": int(status_code),
            "duration_ms": int(fetch_duration_ms),
            "items_fetched": int(items_fetched),
        },
        "emit": {
            "changed": bool(feed.get("changed")),
            "new": len(feed.get("new", [])),
            "updated": len(feed.get("updated", [])),
            "flagged": len(feed.get("flagged", [])),
            "resolved": len(feed.get("resolved", [])),
        },
        "state": {
            "cursor": feed.get("cursor", ""),
            "prev_cursor": feed.get("prev_cursor", ""),
        },
    }
    write_json_atomic(TELEMETRY_PATH, telemetry)


def main() -> None:
    prev_state = read_json(
        STATE_PATH,
        default={"cursor": "init|0", "seen_ids": [], "hash_by_id": {}, "last_run_at": ""}
    )

    fetch_ok = False
    status_code = 0
    items_fetched = 0
    fetch_duration_ms = 0

    try:
        t0 = datetime.now(timezone.utc)
        posts, status_code = fetch_moltbook_posts()
        fetch_duration_ms = int((datetime.now(timezone.utc) - t0).total_seconds() * 1000)
        items_fetched = len(posts)
        fetch_ok = True

        feed, next_state = compute_buckets(prev_state, posts)

        # Write per-source
        write_json_atomic(MOLT_LATEST_PATH, feed)

        # For now, top-level mirrors moltbook (until you add other sources)
        write_json_atomic(TOP_LATEST_PATH, feed)

        # Telemetry (written on success)
        write_telemetry(fetch_ok, status_code, fetch_duration_ms, items_fetched, feed)

        # Clear diffdelta known issues on success
        set_known_issues([])

        # Persist state last
        write_json_atomic(STATE_PATH, next_state)

        print(
            f"OK: changed={feed['changed']} new={len(feed['new'])} updated={len(feed['updated'])} "
            f"flagged={len(feed['flagged'])} cursor={feed['cursor']} fetch_ms={fetch_duration_ms}"
        )

    except Exception as e:
        # Keep known_issues.json schema-valid on error (optional improvement).
        # If you don't want error runs to write known issues, you can remove this block.
        first_seen = now_iso()
        issue = {
            "issue_key": "moltbook_fetch_failed",
            "status": "active",
            "severity": "high",
            "scope": {"level": "source", "ref": SOURCE_NAME},
            "summary": "Moltbook fetch failed; diff feed may be stale.",
            "details": f"{type(e).__name__}: {e}",
            "first_seen_at": first_seen,
            "last_updated_at": now_iso(),
            "signals": ["endpoint"],
            "sources": [
                {"source": SOURCE_NAME, "url": MOLTBOOK_POSTS_URL, "note": "Fetch URL"},
            ],
            "workarounds": [
                {"text": "Retry after TTL; treat feed as stale until next successful run."}
            ],
        }
        try:
            set_known_issues([issue])
        except Exception:
            # Never mask the original error if known_issues writing fails
            pass

        print(f"ERROR: {type(e).__name__}: {e}", file=sys.stderr)
        raise


if __name__ == "__main__":
    main()

