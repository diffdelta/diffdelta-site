#!/usr/bin/env python3
"""
DiffDelta Fleet Engine - Modular source processing system.

Reads sources.config.json and processes deltas for all enabled sources.
Implements Anti-Firehose logic: batch_narrative and risk_score calculation.
"""

import hashlib
import json
import os
import re
import sys
from datetime import datetime, timezone
from typing import Any, Dict, List, Tuple, Optional

import requests

ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
CONFIG_PATH = os.path.join(ROOT, "sources.config.json")
TOP_LATEST_PATH = os.path.join(ROOT, "diff", "latest.json")
KNOWN_ISSUES_PATH = os.path.join(ROOT, "known_issues.json")
TELEMETRY_PATH = os.path.join(ROOT, "telemetry", "latest.json")

# Must satisfy schema constraints
SCHEMA_VERSION = "1.0.0"
GENERATOR_VERSION = "fleet-engine/1.0.0"


# --- time helpers ---

def now_iso() -> str:
    return datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace("+00:00", "Z")


def parse_iso(ts: str) -> Optional[datetime]:
    if not ts or not isinstance(ts, str):
        return None
    try:
        if ts.endswith("Z"):
            ts = ts[:-1] + "+00:00"
        return datetime.fromisoformat(ts)
    except Exception:
        return None


# --- cursor helpers ---

def make_cursor(updated_at: str, stable_id: str) -> str:
    return f"{updated_at}|{stable_id}"


def parse_cursor(cursor: str) -> Tuple[datetime, str]:
    """
    Cursor format: "{iso_ts}|{stable_id}"
    Returns (ts_dt_utc, id_str). Falls back to epoch on bad input.
    """
    if not cursor or not isinstance(cursor, str) or "|" not in cursor:
        return datetime(1970, 1, 1, tzinfo=timezone.utc), ""
    ts, sid = cursor.split("|", 1)
    dt = parse_iso(ts) or datetime(1970, 1, 1, tzinfo=timezone.utc)
    return dt, (sid or "")


def is_after_cursor(updated_at: str, pid: str, boundary_cursor: str) -> bool:
    """
    True if (updated_at, pid) is strictly newer than boundary_cursor.
    Tie-break: updated_at equal -> pid lexicographically greater.
    """
    bdt, bid = parse_cursor(boundary_cursor)
    udt = parse_iso(updated_at) or datetime(1970, 1, 1, tzinfo=timezone.utc)

    if udt > bdt:
        return True
    if udt < bdt:
        return False
    return str(pid) > str(bid)


# --- json io ---

def read_json(path: str, default: Any) -> Any:
    if not os.path.exists(path):
        return default
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def write_json_atomic(path: str, obj: Any) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)
        f.write("\n")
    os.replace(tmp, path)


def sha256(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


# --- risk assessment (Anti-Firehose: risk_score) ---

TOKEN_PATTERNS = [
    re.compile(r"\bsk-[A-Za-z0-9]{20,}\b"),
    re.compile(r"\beyJ[A-Za-z0-9_\-]{10,}\.[A-Za-z0-9_\-]{10,}\.[A-Za-z0-9_\-]{10,}\b"),  # JWT
    re.compile(r"\bAKIA[0-9A-Z]{16}\b"),  # AWS access key id
    re.compile(r"\bgh[pousr]_[A-Za-z0-9_]{20,}\b"),  # GitHub tokens
    re.compile(r"-----BEGIN [A-Z ]*PRIVATE KEY-----"),
]

KEYWORDS = {
    "api_key": "possible_secret",
    "private_key": "possible_secret",
    "api key": "possible_secret",
    "apikey": "possible_secret",
    "access key": "possible_secret",
    "secret key": "possible_secret",
    "private key": "possible_secret",
    "mnemonic": "possible_secret",
    "seed phrase": "possible_secret",
    "bearer": "possible_secret",
    "authorization": "possible_secret",
    "jwt": "possible_secret",

    "supabase": "security_incident_terms",
    "data leak": "security_incident_terms",
    "db leak": "security_incident_terms",
    "database leak": "security_incident_terms",
    "credential leak": "security_incident_terms",
    "key leak": "security_incident_terms",
    "data dump": "security_incident_terms",
    "database dump": "security_incident_terms",
    "credential dump": "security_incident_terms",

    "prompt injection": "prompt_injection_language",
    "ignore previous": "prompt_injection_language",
    "system prompt": "prompt_injection_language",
}


def calculate_risk(raw_item: Dict[str, Any]) -> Tuple[float, List[str]]:
    """
    Calculate risk_score using weighted average of three factors:
    1. Instruction Injection: Look for 'ignore previous' or similar phrases (+0.5 risk)
    2. Keyword Impact: Look for 'deprecated', 'vulnerability', or 'breaking' (+0.3 risk)
    3. Structural Health: If scraper failed to find more than 20% of usual fields (+0.2 risk)
    
    Returns: (risk_score between 0.0 and 1.0, list of risk reasons)
    """
    reasons: List[str] = []
    score = 0.0
    
    # Combine title and content for text analysis
    title = (raw_item.get("title") or "").strip()
    content = (raw_item.get("content") or "").strip()
    combined_text = f"{title}\n{content}".lower()
    
    # Factor 1: Instruction Injection (+0.5 risk)
    instruction_injection_phrases = [
        "ignore previous",
        "ignore all previous",
        "forget previous",
        "disregard previous",
        "override previous",
        "ignore instructions",
        "disregard instructions",
        "new instructions",
        "system prompt",
        "you are now",
        "from now on",
    ]
    
    for phrase in instruction_injection_phrases:
        if phrase in combined_text:
            reasons.append("instruction_injection")
            score += 0.5
            break  # Only count once
    
    # Factor 2: Keyword Impact (+0.3 risk)
    keyword_impact_terms = [
        "deprecated",
        "vulnerability",
        "vulnerable",
        "breaking",
        "breaking change",
        "security issue",
        "security vulnerability",
        "exploit",
        "critical bug",
    ]
    
    for term in keyword_impact_terms:
        if term in combined_text:
            reasons.append("keyword_impact")
            score += 0.3
            break  # Only count once
    
    # Factor 3: Structural Health (+0.2 risk)
    # Define expected fields for a well-formed item
    expected_fields = [
        "id",
        "title",
        "content",
        "author",
        "created_at",
        "updated_at",
        "url",
    ]
    
    # Count how many expected fields are present and non-empty
    present_fields = 0
    for field in expected_fields:
        value = raw_item.get(field)
        # Check if field exists and has a meaningful value
        if value is not None:
            if isinstance(value, str) and value.strip():
                present_fields += 1
            elif isinstance(value, (int, float, bool)):
                present_fields += 1
            elif isinstance(value, dict) and value:
                present_fields += 1
            elif isinstance(value, list) and value:
                present_fields += 1
    
    # Calculate field completeness percentage
    field_completeness = present_fields / len(expected_fields) if expected_fields else 1.0
    
    # If less than 80% of fields are present, add structural health risk
    if field_completeness < 0.8:
        missing_percent = (1.0 - field_completeness) * 100
        reasons.append("structural_health")
        score += 0.2
    
    # Clamp score to [0.0, 1.0]
    score = min(1.0, max(0.0, score))
    
    return score, reasons[:10]


def redact_suspect_secrets(text: str) -> Tuple[str, bool]:
    if not text:
        return text, False

    redacted = False
    out = text

    for pat in TOKEN_PATTERNS:
        if pat.search(out):
            out = pat.sub("[REDACTED]", out)
            redacted = True

    return out, redacted


# --- batch narrative generation (Anti-Firehose: 1-sentence summary) ---

def generate_batch_narrative(
    new_items: List[Dict[str, Any]],
    updated_items: List[Dict[str, Any]],
    source_name: str
) -> str:
    """
    Generate a 1-sentence batch_narrative summarizing all changes.
    Anti-Firehose: Synthesizes 50 updates into a 1-sentence Narrative.
    """
    total_changes = len(new_items) + len(updated_items)
    
    if total_changes == 0:
        return f"No changes detected in {source_name}."
    
    # Extract key themes from titles/summaries
    themes: List[str] = []
    high_risk_count = 0
    
    for item in new_items + updated_items:
        title = item.get("title") or item.get("summary", "")
        if title:
            themes.append(title[:50])  # First 50 chars for theme extraction
        
        risk_score = item.get("risk", {}).get("score", 0.0)
        if risk_score >= 0.7:
            high_risk_count += 1
    
    # Build narrative
    if total_changes == 1:
        item = (new_items + updated_items)[0]
        title = item.get("title") or item.get("summary", "item")
        change_type = "new" if item in new_items else "updated"
        return f"{source_name}: {change_type} '{title[:60]}'."
    
    # Multiple items: synthesize
    new_count = len(new_items)
    updated_count = len(updated_items)
    
    parts = [f"{source_name}: {total_changes} changes"]
    
    if new_count > 0 and updated_count > 0:
        parts.append(f"({new_count} new, {updated_count} updated)")
    elif new_count > 0:
        parts.append(f"({new_count} new)")
    elif updated_count > 0:
        parts.append(f"({updated_count} updated)")
    
    if high_risk_count > 0:
        parts.append(f"with {high_risk_count} high-risk items")
    
    # Add top themes (up to 3)
    unique_themes = list(dict.fromkeys(themes))[:3]
    if unique_themes:
        theme_str = ", ".join([t[:30] for t in unique_themes])
        parts.append(f"including: {theme_str}")
    
    narrative = " ".join(parts) + "."
    
    # Ensure it's a single sentence (max 500 chars for safety)
    if len(narrative) > 500:
        narrative = narrative[:497] + "..."
    
    return narrative


# --- Moltbook adapter ---

class MoltbookAdapter:
    """Adapter for Moltbook source."""
    
    def __init__(self, config: Dict[str, Any], source_name: str):
        self.config = config
        self.source_name = source_name
        self.api_url = config.get("api_url", "")
        self.api_key_env = config.get("api_key_env", "")
        self.max_items = config.get("max_items", 50)
        self.max_summary_len = config.get("max_summary_len", 1200)
    
    def fetch(self) -> Tuple[List[Dict[str, Any]], int]:
        """Fetch posts from Moltbook API."""
        headers = {"Accept": "application/json"}
        key = os.environ.get(self.api_key_env)
        if key:
            headers["Authorization"] = f"Bearer {key}"
        
        r = requests.get(self.api_url, headers=headers, timeout=20)
        status = int(r.status_code)
        r.raise_for_status()
        data = r.json()
        
        # Accept either {posts:[...]} or [...]
        if isinstance(data, dict) and isinstance(data.get("posts"), list):
            return data["posts"], status
        if isinstance(data, list):
            return data, status
        raise ValueError(f"Unexpected Moltbook response shape: {type(data)}")
    
    def best_effort_post_id(self, p: Dict[str, Any]) -> str:
        """Extract stable post ID."""
        for k in ("id", "post_id", "postId"):
            v = p.get(k)
            if v:
                return str(v)
        
        for container_key in ("post", "data", "item"):
            c = p.get(container_key)
            if isinstance(c, dict):
                for k in ("id", "post_id", "postId"):
                    v = c.get(k)
                    if v:
                        return str(v)
        
        # Fall back to stable hash
        url = p.get("url") or ""
        created = p.get("created_at") or p.get("createdAt") or ""
        title = p.get("title") or ""
        content = p.get("content") or ""
        return sha256(f"{url}\n{created}\n{title}\n{content}")[:32]
    
    def best_effort_url(self, p: Dict[str, Any], pid: str) -> str:
        url = p.get("url")
        if isinstance(url, str) and url.startswith("http"):
            return url
        return f"https://www.moltbook.com/post/{pid}"
    
    def best_effort_times(self, p: Dict[str, Any]) -> Tuple[str, str]:
        created = p.get("created_at") or p.get("createdAt")
        updated = p.get("updated_at") or p.get("updatedAt") or created
        
        created_dt = parse_iso(created) if isinstance(created, str) else None
        updated_dt = parse_iso(updated) if isinstance(updated, str) else None
        
        if not created_dt:
            created = now_iso()
        else:
            created = created_dt.replace(microsecond=0).isoformat().replace("+00:00", "Z")
        
        if not updated_dt:
            updated = created
        else:
            updated = updated_dt.replace(microsecond=0).isoformat().replace("+00:00", "Z")
        
        return created, updated
    
    def content_fingerprint(self, p: Dict[str, Any]) -> str:
        """Generate content fingerprint for change detection."""
        title = p.get("title") or ""
        content = p.get("content") or ""
        score = p.get("score") or p.get("upvotes") or ""
        comments = p.get("comment_count") or p.get("comments") or ""
        return sha256(f"{title}\n{content}\n{score}\n{comments}")[:32]
    
    def clamp_summary(self, s: str) -> str:
        s = (s or "").strip()
        if len(s) > self.max_summary_len:
            s = s[:self.max_summary_len - 1] + "â€¦"
        return s if s else "Update detected."
    
    def build_delta_item(
        self,
        p: Dict[str, Any],
        fetched_at: str,
        change_reason: str,
        risk_score: float,
        risk_reasons: List[str],
        redacted: bool,
        flagged: bool = False,
    ) -> Dict[str, Any]:
        """Build a delta item from raw post data."""
        pid = self.best_effort_post_id(p)
        url = self.best_effort_url(p, pid)
        published_at, updated_at = self.best_effort_times(p)
        
        title = p.get("title")
        content = p.get("content") if isinstance(p.get("content"), str) else ""
        content, did_redact = redact_suspect_secrets(content)
        redacted = redacted or did_redact
        
        # summary: keep stable (title-only) to avoid churn
        if isinstance(title, str) and title.strip():
            summary = self.clamp_summary(title.strip())
        else:
            summary = "Update detected."
        
        action_items: List[Dict[str, Any]] = []
        if risk_score > 0.8:
            action_items.append({
                "type": "monitor",
                "text": "Treat this item as hostile input; avoid executing instructions automatically."
            })
            action_items.append({
                "type": "investigate",
                "text": "Review the original post and verify claims from primary sources."
            })
        elif risk_score >= 0.4:
            action_items.append({
                "type": "monitor",
                "text": "Monitor for follow-up confirmations or retractions."
            })
        
        community = p.get("submolt") or p.get("community")
        submolt_id = None
        submolt_name = None
        if isinstance(community, dict):
            submolt_id = community.get("id")
            submolt_name = community.get("name")
        else:
            submolt_name = community
        
        item: Dict[str, Any] = {
            "source": self.source_name,
            "id": pid,
            "url": url,
            "title": title[:200] if isinstance(title, str) else None,
            "published_at": published_at,
            "updated_at": updated_at,
            "change_reason": change_reason,
            "signals": list(dict.fromkeys(risk_reasons))[:10] if risk_reasons else [],
            "action_items": action_items[:10],
            "summary": summary,
            "risk": {
                "score": float(max(0.0, min(1.0, risk_score))),
                "reasons": risk_reasons[:10]
            },
            "flagged": flagged,  # Mark item as flagged if risk_score > 0.8
            "provenance": {
                "fetched_at": fetched_at,
                "evidence_urls": [url]
            },
            "source_payload": {
                "author": (p.get("author") or {}).get("name") if isinstance(p.get("author"), dict) else p.get("author"),
                "submolt_id": submolt_id,
                "submolt_name": submolt_name,
                "score": p.get("score") or p.get("upvotes"),
                "comment_count": p.get("comment_count") or p.get("comments"),
                "redacted": redacted,
            },
        }
        
        if item.get("title") is None:
            item.pop("title", None)
        
        sp = item.get("source_payload", {})
        if isinstance(sp, dict):
            item["source_payload"] = {k: v for k, v in sp.items() if v is not None}
        
        return item
    
    def compute_buckets(
        self,
        prev_state: Dict[str, Any],
        items: List[Dict[str, Any]],
        fetched_at: str,
    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """Compute delta buckets (new, updated, resolved, flagged) from raw items."""
        prev_cursor = prev_state.get("cursor") or "init|0"
        prev_hash_by_id: Dict[str, str] = prev_state.get("hash_by_id") or {}
        
        normalized: List[Dict[str, Any]] = []
        for p in items:
            pid = self.best_effort_post_id(p)
            published_at, updated_at = self.best_effort_times(p)
            fp = self.content_fingerprint(p)
            normalized.append({
                "_raw": p,
                "id": pid,
                "published_at": published_at,
                "updated_at": updated_at,
                "fp": fp,
            })
        
        # Sort newest -> oldest
        normalized.sort(
            key=lambda x: (parse_iso(x["updated_at"]) or datetime.min.replace(tzinfo=timezone.utc), x["id"]),
            reverse=True
        )
        normalized = normalized[:self.max_items]
        
        # Frontier cursor represents newest observed item (even if we emit nothing)
        if normalized:
            frontier_cursor = make_cursor(normalized[0]["updated_at"], normalized[0]["id"])
        else:
            frontier_cursor = prev_cursor
        
        bucket_new: List[Dict[str, Any]] = []
        bucket_updated: List[Dict[str, Any]] = []
        bucket_resolved: List[Dict[str, Any]] = []
        bucket_flagged: List[Dict[str, Any]] = []
        
        next_hash_by_id: Dict[str, str] = dict(prev_hash_by_id)
        next_seen_ids: List[str] = []
        
        changed = False
        
        for row in normalized:
            pid = row["id"]
            p = row["_raw"]
            fp = row["fp"]
            
            # Risk assessment for both New and Updated items (Anti-Firehose)
            risk_score, risk_reasons = calculate_risk(p)
            is_flagged = risk_score > 0.8
            
            # LOGIC A: Is it strictly NEW? (Beyond the cursor)
            if is_after_cursor(row["updated_at"], pid, prev_cursor):
                item = self.build_delta_item(p, fetched_at, "new_item", risk_score, risk_reasons, redacted=False, flagged=is_flagged)
                bucket_new.append(item)
                changed = True
                if is_flagged:
                    bucket_flagged.append(item)
            
            # LOGIC B: Is it an UPDATE? (Behind cursor but fingerprint changed)
            else:
                old_hash = prev_hash_by_id.get(pid)
                if old_hash and fp != old_hash:
                    item = self.build_delta_item(p, fetched_at, "content_changed", risk_score, risk_reasons, redacted=False, flagged=is_flagged)
                    bucket_updated.append(item)
                    changed = True
                    if is_flagged:
                        bucket_flagged.append(item)
            
            # Always track seen IDs and hashes for the next state
            next_seen_ids.append(pid)
            next_hash_by_id[pid] = fp
        
        # Cursor invariant: if changed=false, cursor must equal prev_cursor
        next_cursor = frontier_cursor if changed else prev_cursor
        
        # Keep hash map bounded / relevant
        next_seen_ids = list(dict.fromkeys(next_seen_ids))[:500]
        if next_seen_ids:
            next_hash_by_id = {k: v for k, v in next_hash_by_id.items() if k in set(next_seen_ids)}
        else:
            next_hash_by_id = prev_hash_by_id
        
        # Generate batch_narrative (Anti-Firehose: 1-sentence summary)
        batch_narrative = generate_batch_narrative(bucket_new, bucket_updated, self.source_name)
        
        feed = {
            "schema_version": SCHEMA_VERSION,
            "generated_at": fetched_at,
            "cursor": next_cursor,
            "prev_cursor": prev_cursor,
            "changed": bool(changed),
            "ttl_sec": self.config.get("ttl_sec", 60),
            "sources_included": [self.source_name],
            "batch_narrative": batch_narrative,  # Anti-Firehose: 1-sentence summary
            "new": bucket_new,
            "updated": bucket_updated,
            "resolved": bucket_resolved,
            "flagged": bucket_flagged,
            "meta": {
                "target_max_payload_kb": self.config.get("target_max_payload_kb", 256),
                "generator_version": GENERATOR_VERSION,
            },
        }
        
        next_state = {
            "cursor": next_cursor,
            "seen_ids": next_seen_ids,
            "hash_by_id": next_hash_by_id,
            "last_run_at": fetched_at,
        }
        
        return feed, next_state


# --- adapter registry ---

ADAPTER_REGISTRY = {
    "moltbook": MoltbookAdapter,
}


def get_adapter(adapter_name: str, config: Dict[str, Any], source_name: str):
    """Get adapter instance by name."""
    adapter_class = ADAPTER_REGISTRY.get(adapter_name)
    if not adapter_class:
        raise ValueError(f"Unknown adapter: {adapter_name}")
    return adapter_class(config, source_name)


# --- state management ---

def load_prev_published_cursor(latest_path: str) -> str:
    """Load cursor from previously published feed."""
    prev_feed = read_json(latest_path, default={})
    if isinstance(prev_feed, dict):
        c = prev_feed.get("cursor")
        if isinstance(c, str) and c:
            return c
    return "init|0"


# --- known issues management ---

def set_known_issues(issues: List[Dict[str, Any]]) -> None:
    """Update known_issues.json, avoiding churn if unchanged."""
    new_payload = {
        "schema_version": SCHEMA_VERSION,
        "generated_at": now_iso(),
        "issues": issues,
    }
    
    existing = read_json(KNOWN_ISSUES_PATH, default=None)
    if isinstance(existing, dict):
        if existing.get("schema_version") == new_payload["schema_version"] and existing.get("issues") == issues:
            return
    
    write_json_atomic(KNOWN_ISSUES_PATH, new_payload)


# --- telemetry ---

def write_telemetry(
    source_name: str,
    fetch_ok: bool,
    status_code: int,
    fetch_duration_ms: int,
    items_fetched: int,
    feed: Dict[str, Any],
) -> None:
    """Write telemetry data."""
    run_id = os.environ.get("GITHUB_RUN_ID") or "local"
    telemetry = {
        "schema_version": SCHEMA_VERSION,
        "generated_at": now_iso(),
        "run_id": f"github_actions:{run_id}",
        "source": source_name,
        "fetch": {
            "ok": bool(fetch_ok),
            "status_code": int(status_code),
            "duration_ms": int(fetch_duration_ms),
            "items_fetched": int(items_fetched),
        },
        "emit": {
            "changed": bool(feed.get("changed")),
            "new": len(feed.get("new", [])),
            "updated": len(feed.get("updated", [])),
            "flagged": len(feed.get("flagged", [])),
            "resolved": len(feed.get("resolved", [])),
        },
        "state": {
            "cursor": feed.get("cursor", ""),
            "prev_cursor": feed.get("prev_cursor", ""),
        },
    }
    write_json_atomic(TELEMETRY_PATH, telemetry)


# --- main engine ---

def process_source(source_name: str, source_config: Dict[str, Any]) -> bool:
    """Process a single source. Returns True if successful."""
    if not source_config.get("enabled", False):
        print(f"SKIP: {source_name} is disabled")
        return True
    
    adapter_name = source_config.get("adapter")
    if not adapter_name:
        print(f"ERROR: {source_name} has no adapter specified", file=sys.stderr)
        return False
    
    config = source_config.get("config", {})
    paths = source_config.get("paths", {})
    
    state_path = os.path.join(ROOT, paths.get("state", f"diff/source/{source_name}/_state.json"))
    latest_path = os.path.join(ROOT, paths.get("latest", f"diff/source/{source_name}/latest.json"))
    
    try:
        adapter = get_adapter(adapter_name, config, source_name)
        
        # Load previous state
        prev_state = read_json(
            state_path,
            default={"cursor": "init|0", "seen_ids": [], "hash_by_id": {}, "last_run_at": ""}
        )
        prev_state["cursor"] = load_prev_published_cursor(latest_path)
        
        fetch_ok = False
        status_code = 0
        items_fetched = 0
        fetch_duration_ms = 0
        
        try:
            t0 = datetime.now(timezone.utc)
            items, status_code = adapter.fetch()
            fetch_duration_ms = int((datetime.now(timezone.utc) - t0).total_seconds() * 1000)
            items_fetched = len(items)
            fetch_ok = True
            
            fetched_at = now_iso()
            feed, next_state = adapter.compute_buckets(prev_state, items, fetched_at)
            
            # Only write feeds when changed=true to avoid churn commits
            if feed.get("changed"):
                write_json_atomic(latest_path, feed)
                # Also update top-level aggregate (for now, single source)
                write_json_atomic(TOP_LATEST_PATH, feed)
            
            # Always write telemetry
            write_telemetry(source_name, fetch_ok, status_code, fetch_duration_ms, items_fetched, feed)
            
            # Persist state last
            write_json_atomic(state_path, next_state)
            
            print(
                f"OK [{source_name}]: changed={feed['changed']} new={len(feed['new'])} "
                f"updated={len(feed['updated'])} flagged={len(feed['flagged'])} "
                f"narrative='{feed.get('batch_narrative', 'N/A')[:60]}...'"
            )
            return True
            
        except Exception as e:
            first_seen = now_iso()
            issue = {
                "issue_key": f"{source_name}_fetch_failed",
                "status": "active",
                "severity": "high",
                "scope": {"level": "source", "ref": source_name},
                "summary": f"{source_name} fetch failed; diff feed may be stale.",
                "details": f"{type(e).__name__}: {e}",
                "first_seen_at": first_seen,
                "last_updated_at": now_iso(),
                "signals": ["endpoint"],
                "sources": [
                    {"source": source_name, "url": config.get("api_url", ""), "note": "Fetch URL"},
                ],
                "workarounds": [
                    {"text": "Retry after TTL; treat feed as stale until next successful run."}
                ],
            }
            try:
                set_known_issues([issue])
            except Exception:
                pass
            
            print(f"ERROR [{source_name}]: {type(e).__name__}: {e}", file=sys.stderr)
            return False
            
    except Exception as e:
        print(f"ERROR [{source_name}]: {type(e).__name__}: {e}", file=sys.stderr)
        return False


def main() -> None:
    """Main entry point: load config and process all enabled sources."""
    if not os.path.exists(CONFIG_PATH):
        print(f"ERROR: Config file not found: {CONFIG_PATH}", file=sys.stderr)
        sys.exit(1)
    
    config_data = read_json(CONFIG_PATH, default={})
    sources = config_data.get("sources", {})
    
    if not sources:
        print("WARNING: No sources configured", file=sys.stderr)
        sys.exit(0)
    
    success_count = 0
    fail_count = 0
    
    for source_name, source_config in sources.items():
        if process_source(source_name, source_config):
            success_count += 1
        else:
            fail_count += 1
    
    # Clear known issues if all sources succeeded
    if fail_count == 0:
        set_known_issues([])
    
    if fail_count > 0:
        print(f"\nSUMMARY: {success_count} succeeded, {fail_count} failed", file=sys.stderr)
        sys.exit(1)
    
    print(f"\nSUMMARY: All {success_count} sources processed successfully")


if __name__ == "__main__":
    main()
